services:
  open-webui:
    # image: ghcr.io/open-webui/open-webui:main
    build:
      context: ./openwebui
      dockerfile: Dockerfile
    container_name: open-webui
    restart: always
    ports:
      - "3000:8080"
    volumes:
      - ~/.open-webui:/root/.open-webui
      - ~/.cache/huggingface:/root/.cache/huggingface 
      - ~/.cache/huggingface/hub:/root/.cache/huggingface/hub
      - open-webui:/app/backend/data
      - ~/.ollama/models:/root/.ollama/models
      - ollama:/root/.ollama
    networks:
      - model-hub # when inspecting, will be deepseek_model-hub
    environment:
      OLLAMA_API_BASE_URL: "http://ollama:11434"
      HF_HUB_OFFLINE: "1"
      OFFLINE_MODE: "true"
      DATA_DIR: "/root/.open-webui"
      HF_HOME: "/root/.cache/huggingface"
    extra_hosts:
      - "host.docker.internal:host-gateway"
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    restart: always
    ports:
      - "11434:11434"
    healthcheck:
      test: ollama --version || exit 1
    command: serve
    volumes:
      - ollama:/root/.ollama
    networks:
      - model-hub # when inspecting, will be deepseek_model-hub
volumes:
  open-webui:
  ollama:

networks:
  model-hub: # when inspecting, will be deepseek_model-hub
    driver: bridge

